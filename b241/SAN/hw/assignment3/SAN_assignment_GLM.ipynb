{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7e7f69",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# SAN Assignment - Generalized linear models\n",
    "\n",
    "Author : Your Name\n",
    "\n",
    "Email  : you@fel.cvut.cz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b00804e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Introduction\n",
    "The aim of this assignment is to practice constructing linear models. You will start with a simple linear model. You will evaluate and interpret it (1p). Consequently, your task will be to improve this model using generalized linear models (GLMs) and feature transformations. You will get 1p for proposal and evaluation of GLM (family, evaluation, interpretation), 1p for correct feature transformations, 2p for proposal and justification of the final model and eventually, 1p for comprehensive evaluation of all the model improvements (ablation study through cross-validation, note that the previous evaluations must be done without cross-validation).\n",
    "\n",
    "Submit your solution consisting of both this modified notebook file and the exported PDF/HTML document as an archive to the courseware BRUTE upload system for the SAN course.\n",
    "The deadline is specified there.\n",
    "\n",
    "#### Input data \n",
    "In this assignment, you will work with a student dataset. The dataset contains 200 samples and 4 features: *num_awards* is the outcome variable and indicates the number of awards earned by students in a year, *math* is a continuous predictor variable and represents students’ scores on their math final exam, *prog* is a categorical predictor variable with three levels indicating the type of program in which the students were enrolled (1 = “General”, 2 = “Academic” and 3 = “Vocational”), and *work* is a continuous predictor that gives the number of hours that students spent at work on average per week.\n",
    "\n",
    "#### Load the necessary libraries and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ebbc13-692f-49e1-af20-2574a00d01ad",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "import scipy.stats\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print_precision = 3\n",
    "cut_threshold = 1e5\n",
    "thr = np.format_float_scientific(cut_threshold)\n",
    "formatter_short = (\"{:.\" + f\"{print_precision}\" + \"f}\").format\n",
    "cond_formatter = lambda x: formatter_short(x) if np.abs(x) < cut_threshold else f\">{thr}\"\n",
    "\n",
    "np.set_printoptions(formatter={'float_kind': cond_formatter})\n",
    "pd.options.display.float_format = cond_formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e019650-c6e4-46fa-8068-4990cff8f869",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"study_data.csv\")\n",
    "X = df.loc[:, df.columns != \"num_awards\"]\n",
    "X = sm.add_constant(X)\n",
    "y = df.num_awards.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe169f86",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### Simple linear model\n",
    "Let us start with an ordinary linear model with no feature transformations. Explain how far the model works (does it meet formal assumptions?, does it overcome the null model?). Which predictors would you keep there and which of them are not useful? Use the standard evaluation procedures that we have for linear models (notice, that GML with Gaussian family corresponds to an OLS model, only the summary is slightly more general).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565beb37",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "res = sm.OLS(endog=y, exog=X).fit()\n",
    "print(res.summary())\n",
    "res = sm.GLM(endog=y, exog=X, family=sm.families.Gaussian()).fit()\n",
    "print(res.summary())\n",
    "\n",
    "plt.scatter(y, res.predict(X))\n",
    "plt.ylabel(\"predicted no. awards\")\n",
    "plt.xlabel(\"real no. awards\")\n",
    "plt.ylim((-1, 6))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "try:\n",
    "    sm.graphics.plot_fit(res, \"math\")\n",
    "except AttributeError:\n",
    "    pass\n",
    "finally:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f31ef92",
   "metadata": {},
   "source": [
    "**Add your verbal summary and comments here (1p)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f361458",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Generalized linear model\n",
    "Now, the goal is to implement a generalized linear model that conceptually fits the given task. Do not transform the predictors yet, use them as they are, or omit them from the model. Once you obtain your model, interpret the effect of the *math* predictor on the outcome. How (according to your model) increasing a math score by one point affects the number of awards won? \n",
    "\n",
    "Explain why the model overcomes the previous linear model, or justify that the generalized model is not needed. Compare the models theoretically as well as technically in terms of a proper quality measure(s). Note: The difference between the models generally cannot be statistically tested.\n",
    "\n",
    "Because in Python you cannot use *anova* function refer for example to the [Likelihood ration test (LTR)](https://www.statology.org/likelihood-ratio-test-in-python/) when comparing **nested** models, which you can easily implement yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f61acee",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# TODO: add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d46ee36",
   "metadata": {},
   "source": [
    "**Add your verbal summary and comments here (1p)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b51ed0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Feature transformations and final model\n",
    "*prog* and *work* did not prove to be effective predictors so far. Visualize these predictors as well as their relationship with the outcome variable. Based on the observations, propose suitable transformations for them (or, justify that they are truly uninformative for prediction of *num_awards*) and implement them into the best model found by now. Use the *compareGLM()* function to validate that your new GLMs indeed improved over the simple one.\n",
    "\n",
    "To achieve the same results as with R formulas (e.g. polynomial regression like *num_awards ~ poly(...)*) in Python you need to transform your features explicitly before passing them to your model. You do this by modifying the exog data in X.\n",
    "For tools to do polynomial or spline transformations refer for example to sklearn:\n",
    "[SplineTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.SplineTransformer.html)\n",
    "[PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3f0ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"num_awards\", \"prog\"]\n",
    "df_transformed = df[cols].groupby(cols).size().reset_index(level=-1)\n",
    "programs = pd.unique(df_transformed.prog).tolist()\n",
    "pd.concat([\n",
    "    df_transformed.loc[df_transformed.prog == p, 0]\n",
    "    for p in programs\n",
    "], axis=1).set_axis(programs, axis=\"columns\").plot.bar()\n",
    "plt.title(\"No. of students per no. of awards in each program\")\n",
    "plt.show()\n",
    "\n",
    "for p in programs:\n",
    "    plt.scatter(df.work.loc[df.prog == p], df.num_awards.loc[df.prog == p], label=p)\n",
    "plt.title(\"Number of awards by amount of work and program\")\n",
    "plt.xlabel(\"work\")\n",
    "plt.ylabel(\"awards\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# TODO: add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5256f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Add your verbal summary and comments here (2p)** :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b0111",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Ablation study through cross-validation\n",
    "Recap all the models considered previously and evaluate them through cross-validation. You can start with the most simple null model and gradually add the previously discussed improvements. See their role in terms of MAE, RMSE and other commonly used criteria. The procedure outlined below proposes to work with the preprepared *train_with_cv* function, you can only add more models to evaluate and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2dbe2a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def test_glm_with_cv(X, y, model_type, model_name, model_family=sm.families.Gaussian(), n_folds=10):\n",
    "    \"\"\"\n",
    "    You can use this function to test your GLM models in CV settings. If you want, you can edit\n",
    "    this function, e.g. to have more comparison metrics, but justify it well in your commentary.\n",
    "\n",
    "    :param X: your independent variables / features\n",
    "    :param y: your response variable\n",
    "    :param model_name: just for convenience you can name your models\n",
    "    :param model_family: family of your GLM model (from statsmodels.families),\n",
    "                        the default corresponds to an OLS model\n",
    "    :param n_folds: number of folds to perform in your CV\n",
    "    :return: returns a DataFrame with all the metrics for a particular model\n",
    "    \"\"\"\n",
    "    assert model_type in [\"lm\", \"glm\"]\n",
    "    data_metrics = {\n",
    "        \"MSE\": lambda fit, x, y: mean_squared_error(y, x),\n",
    "        \"MAE\": lambda fit, x, y: mean_absolute_error(y, x),\n",
    "        \"R2\": lambda fit, x, y: r2_score(y, x),\n",
    "        \"LogLikelihood\": lambda fit, x, y: fit.family.loglike_obs(endog=y, mu=x).sum(),\n",
    "    }\n",
    "    model_metrics = {\n",
    "        \"AIC\": lambda fit: fit.aic,\n",
    "        \"BIC\": lambda fit: fit.bic_llf,\n",
    "    }\n",
    "    \n",
    "    skf = KFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "    metrics_results = defaultdict(list)\n",
    "    for train_idxs, test_idxs in skf.split(X, y):\n",
    "        model = sm.GLM(endog=y[train_idxs], exog=X[train_idxs], family=model_family)\n",
    "        fit = model.fit()\n",
    "        \n",
    "        cmp = fit, y[test_idxs], fit.predict(X[test_idxs])\n",
    "        for name, m in data_metrics.items():\n",
    "            metrics_results[name].append(m(*cmp))\n",
    "\n",
    "        for name, m in model_metrics.items():\n",
    "            metrics_results[name].append(m(fit))\n",
    "\n",
    "    res = pd.DataFrame({\n",
    "        \"Mean\": [pd.Series(m).mean() for _, m in metrics_results.items()],\n",
    "        \"Sd\": [pd.Series(m).std() for _, m in metrics_results.items()],\n",
    "    }, index=[n for n, _ in metrics_results.items()])\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058eb3a9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Here you have an example of the null lm model trained and tested with CV\n",
    "\n",
    "# the sm.add_constant prepared a column \"const\" of ones for the intercept estimation\n",
    "print(\"Mean Model Resuts\")\n",
    "print(test_glm_with_cv(\n",
    "    X = X.const.to_numpy(),\n",
    "    y = df.num_awards.to_numpy(),\n",
    "    model_type = \"lm\",\n",
    "    model_name=\"mean_model\"\n",
    "))\n",
    "\n",
    "# TODO: add your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7d24e",
   "metadata": {},
   "source": [
    "**Add your verbal summary and comments here (1p)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614c9b3-c408-4866-9650-90282fc1b579",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Feedback\n",
    "Was some part of the notebook unclear, would any topic need more attention during the tutorials? \n",
    "If you want to leave us feedback on the assignment, we would be happy to hear it. \n",
    "Here is your space:"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "Python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
