{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62009c95-fcdc-473c-905e-784840b61ae5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SAN Assignment - Linear Regression\n",
    "\n",
    "Author : Your Name\n",
    "\n",
    "Email  : you@fel.cvut.cz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9d63f5-4cb2-47b4-bfba-4ea028983a67",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "You have already learned that a regression model may suffer from various issues. When constructing such a model, it is important to check for assumptions such as linearity, independence of errors, normality of residuals, homoscedasticity and the absence of multicollinearity. Additionally, there are other potential issues, such as irrelevant variables, which can reduce model performance. In this assignment you will aim to achieve the best possible linear model using the techniques you have already learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e67aabd-d806-4c70-87cf-a0db397e9844",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Submission\n",
    "Fill in your name above for clarity.\n",
    "To solve this homework, simply write your answers into this document and fill in the marked pieces of code.\n",
    "Submit your solution consisting of both this modified notebook file and the **exported PDF or HTML document** as an archive to the courseware BRUTE upload system for the SAN course.\n",
    "The deadline is specified there. \n",
    "\n",
    "# Initialization\n",
    "\n",
    "Load the required libraries `numpy`, `pandas`, `scipy`, `seaborn`, `matplotlib` and `statsmodels`; make sure you have those installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736368f-6262-4140-af16-fff18e3e60e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from scipy.stats import shapiro\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef80f2-60e0-4343-a388-548159dcf057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A little bit of a nice python implementation of regression diagnostics\n",
    "# Taken from https://www.statsmodels.org/dev/examples/notebooks/generated/linear_regression_diagnostics_plots.html\n",
    "\n",
    "# base code\n",
    "import seaborn as sns\n",
    "import statsmodels\n",
    "from statsmodels.tools.tools import maybe_unwrap_results\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from typing import Type\n",
    "\n",
    "style_talk = 'seaborn-talk'    #refer to plt.style.available\n",
    "\n",
    "class LinearRegDiagnostic():\n",
    "    \"\"\"\n",
    "    Diagnostic plots to identify potential problems in a linear regression fit.\n",
    "    Mainly,\n",
    "        a. non-linearity of data\n",
    "        b. Correlation of error terms\n",
    "        c. non-constant variance\n",
    "        d. outliers\n",
    "        e. high-leverage points\n",
    "        f. collinearity\n",
    "\n",
    "    Authors:\n",
    "        Prajwal Kafle (p33ajkafle@gmail.com, where 3 = r)\n",
    "        Does not come with any sort of warranty.\n",
    "        Please test the code one your end before using.\n",
    "\n",
    "        Matt Spinelli (m3spinelli@gmail.com, where 3 = r)\n",
    "        (1) Fixed incorrect annotation of the top most extreme residuals in\n",
    "            the Residuals vs Fitted and, especially, the Normal Q-Q plots.\n",
    "        (2) Changed Residuals vs Leverage plot to match closer the y-axis\n",
    "            range shown in the equivalent plot in the R package ggfortify.\n",
    "        (3) Added horizontal line at y=0 in Residuals vs Leverage plot to\n",
    "            match the plots in R package ggfortify and base R.\n",
    "        (4) Added option for placing a vertical guideline on the Residuals\n",
    "            vs Leverage plot using the rule of thumb of h = 2p/n to denote\n",
    "            high leverage (high_leverage_threshold=True).\n",
    "        (5) Added two more ways to compute the Cook's Distance (D) threshold:\n",
    "            * 'baseR': D > 1 and D > 0.5 (default)\n",
    "            * 'convention': D > 4/n\n",
    "            * 'dof': D > 4 / (n - k - 1)\n",
    "        (6) Fixed class name to conform to Pascal casing convention\n",
    "        (7) Fixed Residuals vs Leverage legend to work with loc='best'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 results: Type[statsmodels.regression.linear_model.RegressionResultsWrapper]) -> None:\n",
    "        \"\"\"\n",
    "        For a linear regression model, generates following diagnostic plots:\n",
    "\n",
    "        a. residual\n",
    "        b. qq\n",
    "        c. scale location and\n",
    "        d. leverage\n",
    "\n",
    "        and a table\n",
    "\n",
    "        e. vif\n",
    "\n",
    "        Args:\n",
    "            results (Type[statsmodels.regression.linear_model.RegressionResultsWrapper]):\n",
    "                must be instance of statsmodels.regression.linear_model object\n",
    "\n",
    "        Raises:\n",
    "            TypeError: if instance does not belong to above object\n",
    "\n",
    "        Example:\n",
    "        >>> import numpy as np\n",
    "        >>> import pandas as pd\n",
    "        >>> import statsmodels.formula.api as smf\n",
    "        >>> x = np.linspace(-np.pi, np.pi, 100)\n",
    "        >>> y = 3*x + 8 + np.random.normal(0,1, 100)\n",
    "        >>> df = pd.DataFrame({'x':x, 'y':y})\n",
    "        >>> res = smf.ols(formula= \"y ~ x\", data=df).fit()\n",
    "        >>> cls = Linear_Reg_Diagnostic(res)\n",
    "        >>> cls(plot_context=\"seaborn-v0_8-paper\")\n",
    "\n",
    "        In case you do not need all plots you can also independently make an individual plot/table\n",
    "        in following ways\n",
    "\n",
    "        >>> cls = Linear_Reg_Diagnostic(res)\n",
    "        >>> cls.residual_plot()\n",
    "        >>> cls.qq_plot()\n",
    "        >>> cls.scale_location_plot()\n",
    "        >>> cls.leverage_plot()\n",
    "        >>> cls.vif_table()\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(results, statsmodels.regression.linear_model.RegressionResultsWrapper) is False:\n",
    "            raise TypeError(\"result must be instance of statsmodels.regression.linear_model.RegressionResultsWrapper object\")\n",
    "\n",
    "        self.results = maybe_unwrap_results(results)\n",
    "\n",
    "        self.y_true = self.results.model.endog\n",
    "        self.y_predict = self.results.fittedvalues\n",
    "        self.xvar = self.results.model.exog\n",
    "        self.xvar_names = self.results.model.exog_names\n",
    "\n",
    "        self.residual = np.array(self.results.resid)\n",
    "        influence = self.results.get_influence()\n",
    "        self.residual_norm = influence.resid_studentized_internal\n",
    "        self.leverage = influence.hat_matrix_diag\n",
    "        self.cooks_distance = influence.cooks_distance[0]\n",
    "        self.nparams = len(self.results.params)\n",
    "        self.nresids = len(self.residual_norm)\n",
    "\n",
    "    def __call__(self, plot_context='seaborn-v0_8-paper', **kwargs):\n",
    "        # print(plt.style.available)\n",
    "        with plt.style.context(plot_context):\n",
    "            fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10,10))\n",
    "            self.residual_plot(ax=ax[0,0])\n",
    "            self.qq_plot(ax=ax[0,1])\n",
    "            self.scale_location_plot(ax=ax[1,0])\n",
    "            self.leverage_plot(\n",
    "                ax=ax[1,1],\n",
    "                high_leverage_threshold = kwargs.get('high_leverage_threshold'),\n",
    "                cooks_threshold = kwargs.get('cooks_threshold'))\n",
    "            plt.show()\n",
    "\n",
    "        return self.vif_table(), fig, ax,\n",
    "\n",
    "    def residual_plot(self, ax=None):\n",
    "        \"\"\"\n",
    "        Residual vs Fitted Plot\n",
    "\n",
    "        Graphical tool to identify non-linearity.\n",
    "        (Roughly) Horizontal red line is an indicator that the residual has a linear pattern\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        sns.residplot(\n",
    "            x=self.y_predict,\n",
    "            y=self.residual,\n",
    "            lowess=True,\n",
    "            scatter_kws={'alpha': 0.5},\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8},\n",
    "            ax=ax)\n",
    "\n",
    "        # annotations\n",
    "        residual_abs = np.abs(self.residual)\n",
    "        abs_resid = np.flip(np.argsort(residual_abs), 0)\n",
    "        abs_resid_top_3 = abs_resid[:3]\n",
    "        for i in abs_resid_top_3:\n",
    "            ax.annotate(\n",
    "                i,\n",
    "                xy=(self.y_predict[i], self.residual[i]),\n",
    "                color='C3')\n",
    "\n",
    "        ax.set_title('Residuals vs Fitted', fontweight=\"bold\")\n",
    "        ax.set_xlabel('Fitted values')\n",
    "        ax.set_ylabel('Residuals')\n",
    "        return ax\n",
    "\n",
    "    def qq_plot(self, ax=None):\n",
    "        \"\"\"\n",
    "        Standarized Residual vs Theoretical Quantile plot\n",
    "\n",
    "        Used to visually check if residuals are normally distributed.\n",
    "        Points spread along the diagonal line will suggest so.\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        QQ = ProbPlot(self.residual_norm)\n",
    "        fig = QQ.qqplot(line='45', alpha=0.5, lw=1, ax=ax)\n",
    "\n",
    "        # annotations\n",
    "        abs_norm_resid = np.flip(np.argsort(np.abs(self.residual_norm)), 0)\n",
    "        abs_norm_resid_top_3 = abs_norm_resid[:3]\n",
    "        for i, x, y in self.__qq_top_resid(QQ.theoretical_quantiles, abs_norm_resid_top_3):\n",
    "            ax.annotate(\n",
    "                i,\n",
    "                xy=(x, y),\n",
    "                ha='right',\n",
    "                color='C3')\n",
    "\n",
    "        ax.set_title('Normal Q-Q', fontweight=\"bold\")\n",
    "        ax.set_xlabel('Theoretical Quantiles')\n",
    "        ax.set_ylabel('Standardized Residuals')\n",
    "        return ax\n",
    "\n",
    "    def scale_location_plot(self, ax=None):\n",
    "        \"\"\"\n",
    "        Sqrt(Standarized Residual) vs Fitted values plot\n",
    "\n",
    "        Used to check homoscedasticity of the residuals.\n",
    "        Horizontal line will suggest so.\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        residual_norm_abs_sqrt = np.sqrt(np.abs(self.residual_norm))\n",
    "\n",
    "        ax.scatter(self.y_predict, residual_norm_abs_sqrt, alpha=0.5);\n",
    "        sns.regplot(\n",
    "            x=self.y_predict,\n",
    "            y=residual_norm_abs_sqrt,\n",
    "            scatter=False, ci=False,\n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8},\n",
    "            ax=ax)\n",
    "\n",
    "        # annotations\n",
    "        abs_sq_norm_resid = np.flip(np.argsort(residual_norm_abs_sqrt), 0)\n",
    "        abs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]\n",
    "        for i in abs_sq_norm_resid_top_3:\n",
    "            ax.annotate(\n",
    "                i,\n",
    "                xy=(self.y_predict[i], residual_norm_abs_sqrt[i]),\n",
    "                color='C3')\n",
    "\n",
    "        ax.set_title('Scale-Location', fontweight=\"bold\")\n",
    "        ax.set_xlabel('Fitted values')\n",
    "        ax.set_ylabel(r'$\\sqrt{|\\mathrm{Standardized\\ Residuals}|}$');\n",
    "        return ax\n",
    "\n",
    "    def leverage_plot(self, ax=None, high_leverage_threshold=False, cooks_threshold='baseR'):\n",
    "        \"\"\"\n",
    "        Residual vs Leverage plot\n",
    "\n",
    "        Points falling outside Cook's distance curves are considered observation that can sway the fit\n",
    "        aka are influential.\n",
    "        Good to have none outside the curves.\n",
    "        \"\"\"\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        ax.scatter(\n",
    "            self.leverage,\n",
    "            self.residual_norm,\n",
    "            alpha=0.5);\n",
    "\n",
    "        sns.regplot(\n",
    "            x=self.leverage,\n",
    "            y=self.residual_norm,\n",
    "            scatter=False,\n",
    "            ci=False,\n",
    "            lowess=True,\n",
    "            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8},\n",
    "            ax=ax)\n",
    "\n",
    "        # annotations\n",
    "        leverage_top_3 = np.flip(np.argsort(self.cooks_distance), 0)[:3]\n",
    "        for i in leverage_top_3:\n",
    "            ax.annotate(\n",
    "                i,\n",
    "                xy=(self.leverage[i], self.residual_norm[i]),\n",
    "                color = 'C3')\n",
    "\n",
    "        factors = []\n",
    "        if cooks_threshold == 'baseR' or cooks_threshold is None:\n",
    "            factors = [1, 0.5]\n",
    "        elif cooks_threshold == 'convention':\n",
    "            factors = [4/self.nresids]\n",
    "        elif cooks_threshold == 'dof':\n",
    "            factors = [4/ (self.nresids - self.nparams)]\n",
    "        else:\n",
    "            raise ValueError(\"threshold_method must be one of the following: 'convention', 'dof', or 'baseR' (default)\")\n",
    "        for i, factor in enumerate(factors):\n",
    "            label = \"Cook's distance\" if i == 0 else None\n",
    "            xtemp, ytemp = self.__cooks_dist_line(factor)\n",
    "            ax.plot(xtemp, ytemp, label=label, lw=1.25, ls='--', color='red')\n",
    "            ax.plot(xtemp, np.negative(ytemp), lw=1.25, ls='--', color='red')\n",
    "\n",
    "        if high_leverage_threshold:\n",
    "            high_leverage = 2 * self.nparams / self.nresids\n",
    "            if max(self.leverage) > high_leverage:\n",
    "                ax.axvline(high_leverage, label='High leverage', ls='-.', color='purple', lw=1)\n",
    "\n",
    "        ax.axhline(0, ls='dotted', color='black', lw=1.25)\n",
    "        ax.set_xlim(0, max(self.leverage)+0.01)\n",
    "        ax.set_ylim(min(self.residual_norm)-0.1, max(self.residual_norm)+0.1)\n",
    "        ax.set_title('Residuals vs Leverage', fontweight=\"bold\")\n",
    "        ax.set_xlabel('Leverage')\n",
    "        ax.set_ylabel('Standardized Residuals')\n",
    "        plt.legend(loc='best')\n",
    "        return ax\n",
    "\n",
    "    def vif_table(self):\n",
    "        \"\"\"\n",
    "        VIF table\n",
    "\n",
    "        VIF, the variance inflation factor, is a measure of multicollinearity.\n",
    "        VIF > 5 for a variable indicates that it is highly collinear with the\n",
    "        other input variables.\n",
    "        \"\"\"\n",
    "        vif_df = pd.DataFrame()\n",
    "        vif_df[\"Features\"] = self.xvar_names\n",
    "        vif_df[\"VIF Factor\"] = [variance_inflation_factor(self.xvar, i) for i in range(self.xvar.shape[1])]\n",
    "\n",
    "        return (vif_df\n",
    "                .sort_values(\"VIF Factor\")\n",
    "                .round(2))\n",
    "\n",
    "\n",
    "    def __cooks_dist_line(self, factor):\n",
    "        \"\"\"\n",
    "        Helper function for plotting Cook's distance curves\n",
    "        \"\"\"\n",
    "        p = self.nparams\n",
    "        formula = lambda x: np.sqrt((factor * p * (1 - x)) / x)\n",
    "        x = np.linspace(0.001, max(self.leverage), 50)\n",
    "        y = formula(x)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "    def __qq_top_resid(self, quantiles, top_residual_indices):\n",
    "        \"\"\"\n",
    "        Helper generator function yielding the index and coordinates\n",
    "        \"\"\"\n",
    "        offset = 0\n",
    "        quant_index = 0\n",
    "        previous_is_negative = None\n",
    "        for resid_index in top_residual_indices:\n",
    "            y = self.residual_norm[resid_index]\n",
    "            is_negative = y < 0\n",
    "            if previous_is_negative == None or previous_is_negative == is_negative:\n",
    "                offset += 1\n",
    "            else:\n",
    "                quant_index -= offset\n",
    "            x = quantiles[quant_index] if is_negative else np.flip(quantiles, 0)[quant_index]\n",
    "            quant_index += 1\n",
    "            previous_is_negative = is_negative\n",
    "            yield resid_index, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6557de5-7849-45c5-9496-50f7b0e8772e",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Input data\n",
    "\n",
    "In this assignment, you will work with a dataset that contains 200 samples, 10 features x_1,,x_{10} and one dependent variable y. Your general task is to develop the best possible linear model. One that is both simple and interpretable while still performing well. You are expected to correctly interpret the model. Assume that outliers are not expected, and our primary focus is on identifying common trends rather than a few exceptional samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f38510-79da-4dbe-9c34-5d7c8aeb1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"lreg_data.csv\")\n",
    "n_samples = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f39dd-ec36-496a-a258-64288cf426ad",
   "metadata": {},
   "source": [
    "# The full linear model\n",
    "Let us start with a full linear model. The first task is to evaluate the model based on its fisrt summary. Then, check its assumptions and detect its potential issues. Your task is to evaluate the plots and tests performed below, you should also further develop them (propose their alternatives).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171b8290-bc5b-4a64-b45e-45bb95beafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lm = smf.ols('y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10', data=data).fit()\n",
    "print(full_lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b168110-9fe1-434a-89c0-3aa6a1266871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot\n",
    "lm_diag = LinearRegDiagnostic(full_lm)\n",
    "lm_diag.residual_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729fc8ea-5b58-43d2-bf5d-e451a8eb483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homoscedasticity test\n",
    "_, pval, __, ___ = het_breuschpagan(full_lm.resid, full_lm.model.exog)\n",
    "print(f\"Breusch-Pagan p-value: {pval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a89fc5-83ca-4a11-ab01-1b84cd03ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality of residuals - Q-Q plot and Shapiro-Wilk test\n",
    "lm_diag.qq_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58086a-20c2-40b1-be3c-3dd36975fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shapiro(full_lm.resid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04306fc6-895f-4159-afee-3d91671c952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers detection using Cook's distance\n",
    "influence = full_lm.get_influence()\n",
    "cooks_d = influence.cooks_distance[0]\n",
    "outliers = np.where(cooks_d > 4 / len(data))[0]\n",
    "print(f\"Outliers: {outliers}\")\n",
    "\n",
    "x=np.arange(n_samples)\n",
    "ax = sns.barplot(y=cooks_d, x=x)\n",
    "ax.plot(x, np.full(n_samples, 4 / len(data)), linestyle=\"dotted\", linewidth=0.5)\n",
    "for o in outliers:\n",
    "    ax.text(x[o] + 1, cooks_d[o] + 0.005, str(o))\n",
    "ax.set_xticks(range(0, 250, 50))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb122ea-ddb5-4c50-b338-5d868c63332f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance Inflation Factor for multicollinearity detection\n",
    "lm_diag.vif_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142009c0-916f-43a2-960c-9ed442b555c3",
   "metadata": {},
   "source": [
    "### **Add your verbal summary here (1p)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919755b5-be5a-40a7-a508-0aa318405932",
   "metadata": {},
   "source": [
    "# Remove the issues from the full model\n",
    "\n",
    "Now deal with the detected issues. Build an alternative model and show that it works better than the full model. Finally, verbally justify all the decisions that you made and explain their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87a3c8-fbd1-44de-9157-80430faacb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### add your code here (1p) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baadf040-c94e-4d46-a159-495c487d8716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab4476ce-e1f5-48ca-bd37-8413a0552d7f",
   "metadata": {},
   "source": [
    "### **Add your verbal summary here (1p)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a394c3-4d3f-4585-bd01-233077bdee87",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "\n",
    "Now, the goal is to run feature selection and remove irrelevant variables. Compare several different feature selection methods and pick one of them. Demonstrate that feature selection applied to the full model brings very different results than in the alternative model without issues. At the end, explain the model.\n",
    "\n",
    "\n",
    "To see, how you can achieve regularization of your model refer to [documentation](https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.fit_regularized.html#statsmodels.regression.linear_model.OLS.fit_regularizeddocumentation).\n",
    "LASSO could be implemented, for example, by:\n",
    "```\n",
    "model = sma.OLS(endog=y, exog=X)\n",
    "fit = model.fit_regularized(alpha=a, L1_wt=1) \n",
    "```\n",
    "Note, that you need to set two parameters: alpha, but also L1_wt, which switches different kinds of regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a588b-6484-47d6-bb15-6f170e621a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### add your code here (1p) ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b8caf-5896-4b95-a76c-050537ef5694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2a8898c-d782-4683-a565-498d4da7e57b",
   "metadata": {},
   "source": [
    "### **Add your verbal summary here (1p)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c7b038-3bd6-4c84-9898-386efa404fe1",
   "metadata": {},
   "source": [
    "## Grading \n",
    "+ 1p for selection of proper methods for assumption verification and their explanation\n",
    "+ 1p for the correct implementation of issue removal\n",
    "+ 1p for its explanation and right decisions in further model design themselves\n",
    "+ 1p for proper application of feature selection techniques and their comparison \n",
    "+ 1p for comprehensive evaluation of all the model improvements and the final wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17664db4-5446-48c5-942d-80ca26ce2821",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "Was some part of the notebook unclear, would any topic need more attention during the tutorials? \n",
    "If you want to leave us feedback on the assignment, we would be happy to hear it. \n",
    "Here is your space:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:san]",
   "language": "python",
   "name": "conda-env-san-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
