{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa3916d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# SAN Assignment - Linear discriminant analysis\n",
    "\n",
    "Author : Your Name\n",
    "\n",
    "Email  : you@fel.cvut.cz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2407c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introduction\n",
    "The aim of this assignment is to get familiar with Linear Discriminant Analysis\n",
    "(LDA). LDA and Principal Component Analysis (PCA) are two techniques\n",
    "for dimensionality reduction. PCA can be described as an unsupervised algorithm that ignores data labels and aims to find directions which maximalize\n",
    "the variance in a data. In comparison with PCA, LDA is a supervised algorithm and aims to project a dataset onto a lower dimensional space with good\n",
    "class separability. In other words, LDA maximalizes the ratio of betweenclass variance and the within-class variance in a given data.\n",
    "\n",
    "Your task is to complete prepared LDA implementation and inspect the mehods behavior. \n",
    "See **all** sections with `# TODO:`. \n",
    "Submit your solution consisting of both this modified notebook file and the exported PDF/HTML document as an archive to the courseware BRUTE upload system for the SAN course.\n",
    "The deadline is specified there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a3e363",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Input data \n",
    "In this tutorial, we will work with a dataset that classifies wines (samples)\n",
    "into three classes using of 13 continuous attributes; for more details see\n",
    "wine info.txt file. The dataset is located at wine.csv.\n",
    "\n",
    "## Linear Discriminant Analysis\n",
    "As we mentioned above, LDA finds directions where classes are well-separated,\n",
    "i.e. LDA maximizes the ratio of between-class variance and the within-class\n",
    "variance. Firstly, assume that $C$ is a set of classes and set $D$, which represents\n",
    "a training dataset, is defined as $D = \\{x_1, x_2, . . . , x_N \\}$.\n",
    "\n",
    "The between-classes scatter matrix SB is defined as:\n",
    "$S_b = \\sum_c N_C(\\mu_c -\\overline{x})(\\mu_c - \\overline{x})^T$, where $\\overline{x}$ is a vector represents the overall mean of the data, Âµ represents the mean corresponding to each class, and $N_C$ are sizes of the respective classes.\n",
    "\n",
    "The within-classes scatter matrix $S_W$ is defined as:\n",
    "\n",
    "$S_W = \\sum_c \\sum_{x \\in D_c}(x - \\overline{\\mu_c})(x - \\overline{\\mu_c})^T$\n",
    "\n",
    "Next, we will solve the generalized eigenvalue problem for the matrix $S_W^{-1}S_B$ to obtain the linear discriminants, i.e.\n",
    "\n",
    "$(S_W^{-1}S_B)w = \\lambda w$\n",
    "\n",
    "where $w$ represents an eigenvector and $\\lambda$ represents an eigenvalue. Finally,\n",
    "choose k eigenvectors with the largest eigenvalue and transform the samples\n",
    "onto the new subspace.\n",
    "\n",
    "\n",
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfea077",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280fd84c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step by step\n",
    "\n",
    "In this assignment, you first finish the given LDA implementation and then compare it with PCA (for a classification task).\n",
    "\n",
    "\n",
    "### LDA Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c8f8b3",
   "metadata": {
    "lines_to_next_cell": 0,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Compute the within-scatter matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a46ad6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_within_scatter(data):\n",
    "    \"\"\" Function to compute within scatter matrix S_w.\n",
    "\n",
    "    :param data: list of numpy arrays with data from individual classes\n",
    "    \"\"\"\n",
    "    # TODO: replace with your code\n",
    "    within_matrix = np.eye(data[0].shape[1])\n",
    "\n",
    "    return within_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caefb32",
   "metadata": {
    "lines_to_next_cell": 0,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Compute the between-scatter matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b56fa",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_between_scatter(data):\n",
    "    \"\"\" Function to compute between scatter matrix S_b.\n",
    "\n",
    "    :param data: list of numpy arrays with data from individual classes\n",
    "    \"\"\"\n",
    "    # TODO: replace with your code\n",
    "    between_matrix = np.eye(data[0].shape[1])\n",
    "\n",
    "    return between_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c784202b",
   "metadata": {
    "lines_to_next_cell": 0,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Solve the EigenProblem and return eigen-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bde079",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def solve_eigen_problem(within_matrix, between_matrix):\n",
    "    \"\"\" Compute eigenvectors as defined by the method.\n",
    "\n",
    "    :param within_matrix: numpy array (n_variables, n_variables)\n",
    "    :param between_matrix: numpy array, same shape\n",
    "    :return: real eigenvectors, ordered from the highest eigenvalue, numpy array, (n_variables,n)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: replace with your code\n",
    "    eigenvectors = None\n",
    "\n",
    "    return eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6841d4b3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Visualize the results\n",
    "Project your data into lower-dimensional subspace, visualize this projection, and compare with PCA (see Fig. 1). Use the following code while filling in the lines marked as `TODO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d5cd81",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_by_class(data, labels):\n",
    "    ret = []\n",
    "    for lbl in np.unique(labels):\n",
    "        ret.append(data[labels == lbl])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def compute_centroids(data, labels):\n",
    "    ret = []\n",
    "    for lbl in np.unique(labels):\n",
    "        ret.append(data[labels == lbl].mean(axis=0))\n",
    "    return np.array(ret)\n",
    "\n",
    "\n",
    "def classify(data, eigen_vectors, centroids, prior):\n",
    "    n_classes = len(centroids)\n",
    "\n",
    "    y = data @ eigen_vectors[:, :n_classes]\n",
    "    classif = np.empty((data.shape[0], n_classes))\n",
    "    for c in range(n_classes):\n",
    "        classif[:, c] = ((y - centroids[c]) ** 2 / 2).sum(axis=1) - np.log(prior[c])\n",
    "    return classif.argmin(axis=1) + 1  # indexes start 0, labels start as 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd87555",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def LDA(data, labels, plot_title=None):\n",
    "    data_per_class = split_by_class(data, labels)\n",
    "\n",
    "    # 1) scatter matrices\n",
    "    # within-class scatter matrix Sw\n",
    "    S_w = compute_within_scatter(data_per_class)\n",
    "\n",
    "    # between-class scatter matrix Sb\n",
    "    S_b = compute_between_scatter(data_per_class)\n",
    "\n",
    "    # 2)  eigen problem\n",
    "    # solve eigen problem\n",
    "    eigenvectors = solve_eigen_problem(S_w, S_b)\n",
    "\n",
    "    #transform the samples onto the new subspace\n",
    "    y = data @ eigenvectors[:, :2]\n",
    "\n",
    "    ## visual comparison with PCA\n",
    "    scaled_data = StandardScaler().fit_transform(data)\n",
    "    y_pca = PCA(2).fit_transform(data)\n",
    "    y_pca_scaled = PCA(2).fit_transform(scaled_data)\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=3, figsize=(12, 6))\n",
    "    axs[0].scatter(y[:, 0], y[:, 1], c=labels)\n",
    "    axs[0].set_xlabel(\"LDA_1\")\n",
    "    axs[0].set_ylabel(\"LDA_2\")\n",
    "    axs[0].set_title(\"LDA\")\n",
    "\n",
    "    axs[1].scatter(y_pca[:, 0], y_pca[:, 1], c=labels)\n",
    "    axs[1].set_xlabel(\"PCA_1\")\n",
    "    axs[1].set_ylabel(\"PCA_2\")\n",
    "    axs[1].set_title(\"PCA not scaled\")\n",
    "\n",
    "    axs[2].scatter(y_pca_scaled[:, 0], y_pca_scaled[:, 1], c=labels)\n",
    "    axs[2].set_xlabel(\"PCA_1\")\n",
    "    axs[2].set_ylabel(\"PCA_2\")\n",
    "    axs[2].set_title(\"PCA scaled\")\n",
    "\n",
    "    if plot_title is not None:\n",
    "        fig.suptitle(plot_title)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # fig.show()  # in non-interactive fashion, this gives non-useful warning\n",
    "\n",
    "    return eigenvectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda53a1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def crossvalidation_lda(data, labels, n_folds=10):\n",
    "    seed = 0\n",
    "    n_classes = len(np.unique(labels))\n",
    "    accuracy = []\n",
    "    skf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for i, (train_idxs, test_idxs) in enumerate(skf.split(data, labels)):\n",
    "        eigen_lda = LDA(data[train_idxs], labels[train_idxs], plot_title=f\"CV Split {i}\")\n",
    "        projected_data = data[train_idxs] @ eigen_lda[:, :n_classes]\n",
    "        centroids = compute_centroids(projected_data, labels[train_idxs])\n",
    "        predicted_labels = classify(data[test_idxs], eigen_lda, centroids,\n",
    "                                    np.full(n_classes, 1. / n_classes))\n",
    "        accuracy.append(np.mean(predicted_labels == labels[test_idxs]))\n",
    "    return np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3141442-8c4f-4f05-8937-53182c5efd96",
   "metadata": {},
   "source": [
    "### Run your LDA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03991d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1. read the data\n",
    "df = pd.read_csv(\"wine.csv\", header=None)\n",
    "labels = df.iloc[:, 0].values\n",
    "data = df.iloc[:, 1:].values\n",
    "n_classes = len(np.unique(labels))\n",
    "\n",
    "# 2. compute LDA and return corresponding eigenvectors\n",
    "eigen_lda = LDA(data, labels, plot_title=\"All data\")\n",
    "\n",
    "# 3. project data on LDA defined space\n",
    "# TODO: replace with your own solution\n",
    "projected_data = None\n",
    "\n",
    "# 4. find centroids and assign data to them (classify)\n",
    "centroids = compute_centroids(projected_data, labels)\n",
    "predicted_labels = classify(data, eigen_lda, centroids, np.full(n_classes, 1. / n_classes))\n",
    "\n",
    "# 5. evaluate the algorithm\n",
    "# ACC\n",
    "print(\"accuracy:\", np.mean(predicted_labels == labels))\n",
    "\n",
    "# cross-validation\n",
    "acc_lda = crossvalidation_lda(data, labels, n_folds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6ff71",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Discuss your results.** Compare your LDA performance with PCA, explain how the two methods differ theoretically. What else would you conclude from your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96258c-0f0d-48d6-b270-71a1f02f92cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca858e51-6b00-4ce8-8e06-e34deddf2327",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Feedback\n",
    "Was some part of the notebook unclear, would any topic need more attention during the tutorials? \n",
    "If you want to leave us feedback on the assignment, we would be happy to hear it. \n",
    "Here is your space:"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "Python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
